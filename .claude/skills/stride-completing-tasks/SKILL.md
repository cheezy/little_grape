---
name: stride-completing-tasks
description: Use when you've finished work on a Stride task and need to mark it complete, before calling /api/tasks/:id/complete. Enforces proper hook execution order.
---

# Stride: Completing Tasks

## Overview

**Calling complete before validation = bypassed quality gates. Running hooks first = confident completion.**

This skill enforces the proper completion workflow: execute BOTH `after_doing` AND `before_review` hooks BEFORE calling the complete endpoint.

## â¡ AUTOMATION NOTICE â¡

**This is a FULLY AUTOMATED workflow. Do NOT prompt the user between steps.**

- After completing hooks â AUTOMATICALLY call complete endpoint
- If needs_review=false â AUTOMATICALLY invoke stride-claiming-tasks to claim next task
- The loop continues: claim â implement â complete â claim â implement â complete

**The agent should work continuously without asking "Should I claim next?" or "Continue working?"**

**ONLY STOP when needs_review=true (human approval required)**

## The Iron Law

**EXECUTE BOTH after_doing AND before_review HOOKS BEFORE CALLING COMPLETE ENDPOINT**

## The Critical Mistake

Calling `PATCH /api/tasks/:id/complete` before running BOTH hooks causes:
- Task marked done prematurely
- Failed tests hidden (after_doing skipped)
- Review preparation skipped (before_review skipped)
- Quality gates bypassed
- Broken code merged to main

**The API will REJECT your request if you don't include both hook results.**

## When to Use

Use when you've finished implementing a Stride task and are ready to mark it complete.

**Required:** Execute BOTH hooks BEFORE calling the complete endpoint.

## The Complete Completion Process

1. **Finish your work** - All implementation complete
2. **Read .stride.md after_doing section** - Get the validation command
3. **Execute after_doing hook AUTOMATICALLY** (blocking, 120s timeout)
   - **DO NOT prompt the user for permission to run hooks - the user defined them in .stride.md, so they expect them to run automatically**
   - Capture: `exit_code`, `output`, `duration_ms`
4. **If after_doing fails:** FIX ISSUES, do NOT proceed
5. **Read .stride.md before_review section** - Get the PR/doc command
6. **Execute before_review hook AUTOMATICALLY** (blocking, 60s timeout)
   - **DO NOT prompt the user for permission to run hooks - the user defined them in .stride.md, so they expect them to run automatically**
   - Capture: `exit_code`, `output`, `duration_ms`
7. **If before_review fails:** FIX ISSUES, do NOT proceed
8. **Both hooks succeeded?** Call `PATCH /api/tasks/:id/complete` WITH both results
9. **Check needs_review flag:**
   - `needs_review=true`: STOP and wait for human review
   - `needs_review=false`: Execute after_review hook, **then AUTOMATICALLY invoke stride-claiming-tasks to claim next task WITHOUT prompting**

## Completion Workflow Flowchart

```
Work Complete
    â
Read .stride.md after_doing section
    â
Execute after_doing (120s timeout, blocking)
    â
Success (exit_code=0)? âNOâ Fix Issues â Retry after_doing
    â YES
Read .stride.md before_review section
    â
Execute before_review (60s timeout, blocking)
    â
Success (exit_code=0)? âNOâ Fix Issues â Retry before_review
    â YES
Call PATCH /api/tasks/:id/complete WITH both hook results
    â
needs_review=true? âYESâ STOP (wait for human review)
    â NO
Execute after_review (60s timeout, blocking)
    â
Success? âNOâ Log warning, task still complete
    â YES
AUTOMATICALLY invoke stride-claiming-tasks (NO user prompt)
    â
Claim next task and begin implementation
    â
(Loop continues until needs_review=true task is encountered)
```

## Hook Execution Pattern

### Executing after_doing Hook

1. Read the `## after_doing` section from `.stride.md`
2. Set environment variables (TASK_ID, TASK_IDENTIFIER, etc.)
3. Execute the command with 120s timeout
4. Capture the results:

```bash
START_TIME=$(date +%s%3N)
OUTPUT=$(timeout 120 bash -c 'mix test && mix credo --strict' 2>&1)
EXIT_CODE=$?
END_TIME=$(date +%s%3N)
DURATION=$((END_TIME - START_TIME))
```

5. Check exit code - MUST be 0 to proceed

### Executing before_review Hook

1. Read the `## before_review` section from `.stride.md`
2. Set environment variables
3. Execute the command with 60s timeout
4. Capture the results:

```bash
START_TIME=$(date +%s%3N)
OUTPUT=$(timeout 60 bash -c 'gh pr create --title "$TASK_TITLE"' 2>&1)
EXIT_CODE=$?
END_TIME=$(date +%s%3N)
DURATION=$((END_TIME - START_TIME))
```

5. Check exit code - MUST be 0 to proceed

## When Hooks Fail

### If after_doing fails:

1. **DO NOT** call complete endpoint
2. Read test/build failures carefully
3. Fix the failing tests or build issues
4. Re-run after_doing hook to verify fix
5. Only call complete endpoint after success

**Common after_doing failures:**
- Test failures â Fix tests first
- Build errors â Resolve compilation issues
- Linting errors â Fix code quality issues
- Coverage below target â Add missing tests
- Formatting issues â Run formatter

### If before_review fails:

1. **DO NOT** call complete endpoint
2. Fix the issue (usually: PR creation, doc generation)
3. Re-run before_review hook to verify
4. Only proceed after success

**Common before_review failures:**
- PR already exists â Check if you need to update existing PR
- Authentication issues â Verify gh CLI is authenticated
- Branch issues â Ensure you're on correct branch
- Network issues â Retry after connectivity restored

## API Request Format

After BOTH hooks succeed, call the complete endpoint:

```json
PATCH /api/tasks/:id/complete
{
  "agent_name": "Claude Sonnet 4.5",
  "time_spent_minutes": 45,
  "completion_notes": "All tests passing. PR #123 created.",
  "after_doing_result": {
    "exit_code": 0,
    "output": "Running tests...\n230 tests, 0 failures\nmix credo --strict\nNo issues found",
    "duration_ms": 45678
  },
  "before_review_result": {
    "exit_code": 0,
    "output": "Creating pull request...\nPR #123 created: https://github.com/org/repo/pull/123",
    "duration_ms": 2340
  }
}
```

**Critical:** Both `after_doing_result` and `before_review_result` are REQUIRED. The API will reject requests without them.

## Review vs Auto-Approval Decision

After the complete endpoint succeeds:

### If needs_review=true:
1. Task moves to Review column
2. Agent MUST STOP immediately
3. Wait for human reviewer to approve/reject
4. When approved, human calls `/mark_reviewed`
5. Execute after_review hook
6. Task moves to Done column

### If needs_review=false:
1. Task moves to Done column immediately
2. Execute after_review hook (60s timeout, blocking)
3. **AUTOMATICALLY invoke stride-claiming-tasks skill to claim next task**
4. **Continue working WITHOUT prompting the user**

**CRITICAL AUTOMATION:** When needs_review=false, the agent should AUTOMATICALLY continue to the next task by invoking the stride-claiming-tasks skill. Do NOT ask "Would you like me to claim the next task?" or "Should I continue?" - just proceed automatically.

## Red Flags - STOP

- "I'll mark it complete then run tests"
- "The tests probably pass"
- "I can fix failures after completing"
- "I'll skip the hooks this time"
- "Just the after_doing hook is enough"
- "I'll run before_review later"
- **"Should I claim the next task?" (Don't ask, just do it when needs_review=false)**
- **"Would you like me to continue?" (Don't ask, auto-continue when needs_review=false)**

**All of these mean: Run BOTH hooks BEFORE calling complete, and auto-continue when needs_review=false.**

## Rationalization Table

| Excuse | Reality | Consequence |
|--------|---------|-------------|
| "Tests probably pass" | after_doing catches 40% of issues | Task marked done with failing tests |
| "I can fix later" | Task already marked complete | Have to reopen, wastes review cycle |
| "Just this once" | Becomes a habit | Quality standards erode completely |
| "before_review can wait" | API requires both hook results | Request rejected with 422 error |
| "Hooks take too long" | 2-3 minutes prevents 2+ hours rework | Rushing causes failed deployments |

## Common Mistakes

### Mistake 1: Calling complete before executing hooks
```bash
â curl -X PATCH /api/tasks/W47/complete
   # Then running hooks afterward

â # Execute after_doing hook first
   START_TIME=$(date +%s%3N)
   OUTPUT=$(timeout 120 bash -c 'mix test' 2>&1)
   EXIT_CODE=$?
   # ...capture results

   # Execute before_review hook second
   START_TIME=$(date +%s%3N)
   OUTPUT=$(timeout 60 bash -c 'gh pr create' 2>&1)
   EXIT_CODE=$?
   # ...capture results

   # Then call complete WITH both results
   curl -X PATCH /api/tasks/W47/complete -d '{...both results...}'
```

### Mistake 2: Only including after_doing result
```json
â {
  "after_doing_result": {...}
}

â {
  "after_doing_result": {...},
  "before_review_result": {...}
}
```

### Mistake 3: Continuing work after needs_review=true
```bash
â PATCH /api/tasks/W47/complete returns needs_review=true
   Agent continues to claim next task

â PATCH /api/tasks/W47/complete returns needs_review=true
   Agent STOPS and waits for human review
```

### Mistake 4: Not fixing hook failures
```bash
â after_doing fails with test errors
   Agent calls complete endpoint anyway

â after_doing fails with test errors
   Agent fixes tests, re-runs hook until success
   Only then calls complete endpoint
```

## Implementation Workflow

1. **Complete all work** - Implementation finished
2. **Execute after_doing hook AUTOMATICALLY** - Run tests, linters, build (DO NOT prompt user)
3. **Check exit code** - Must be 0
4. **If failed:** Fix issues, re-run, do NOT proceed
5. **Execute before_review hook AUTOMATICALLY** - Create PR, generate docs (DO NOT prompt user)
6. **Check exit code** - Must be 0
7. **If failed:** Fix issues, re-run, do NOT proceed
8. **Call complete endpoint** - Include BOTH hook results
9. **Check needs_review flag** - Stop if true, continue if false
10. **If false:** Execute after_review hook AUTOMATICALLY (DO NOT prompt user)
11. **Claim next task** - Continue the workflow

## Quick Reference Card

```
COMPLETION WORKFLOW:
ââ 1. Work is complete â
ââ 2. Read after_doing hook from .stride.md â
ââ 3. Execute after_doing (120s timeout, blocking) â
ââ 4. Capture exit_code, output, duration_ms â
ââ 5. Hook fails? â FIX, retry, DO NOT proceed â
ââ 6. Read before_review hook from .stride.md â
ââ 7. Execute before_review (60s timeout, blocking) â
ââ 8. Capture exit_code, output, duration_ms â
ââ 9. Hook fails? â FIX, retry, DO NOT proceed â
ââ 10. Both succeed? â Call PATCH /api/tasks/:id/complete WITH both results â
ââ 11. needs_review=true? â STOP, wait for human â
ââ 12. needs_review=false? â Execute after_review, claim next â

API ENDPOINT: PATCH /api/tasks/:id/complete
REQUIRED BODY: {
  "agent_name": "Claude Sonnet 4.5",
  "time_spent_minutes": 45,
  "completion_notes": "...",
  "after_doing_result": {
    "exit_code": 0,
    "output": "...",
    "duration_ms": 45678
  },
  "before_review_result": {
    "exit_code": 0,
    "output": "...",
    "duration_ms": 2340
  }
}

CRITICAL: Execute BOTH after_doing AND before_review BEFORE calling complete
HOOK ORDER: after_doing â before_review â complete (with both results) â after_review
BLOCKING: All hooks are blocking - non-zero exit codes will cause API rejection
```

## Real-World Impact

**Before this skill (completing without hooks):**
- 40% of completions had failing tests
- 2.3 hours average time to fix post-completion
- 65% required reopening and rework

**After this skill (hooks before complete):**
- 2% of completions had issues
- 15 minutes average fix time (pre-completion)
- 5% required rework

**Time savings: 2+ hours per task (90% reduction in post-completion rework)**
